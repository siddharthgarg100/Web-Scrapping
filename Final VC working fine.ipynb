{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "findata = open(r'C:\\Users\\DELL\\Downloads\\general.csv', 'w')\n",
    "findata2 = open(r'C:\\Users\\DELL\\Downloads\\general2.csv', 'w')\n",
    "\n",
    "\n",
    "class scraping:\n",
    "    \n",
    "    def __init__(self):\n",
    "        'Constructor to Initiate Selenium and Chrome'\n",
    "        self.options=webdriver.ChromeOptions()\n",
    "        self.options.add_argument('--user-data-dir=C:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Google\\\\Chrome\\\\User Data')\n",
    "\n",
    "        self.browser=webdriver.Chrome(executable_path=r'C:/Users/DELL/Downloads/chromedriver_win32/chromedriver.exe')   \n",
    "        self.header=dict()\n",
    "        self.content=dict()\n",
    "        self.last_company_id=0\n",
    "        \n",
    "    def openWebsite(self):\n",
    "        'Method to open Website and login, Search and call scrap method'         \n",
    "        self.browser.wait = WebDriverWait(self.browser, 20)\n",
    "        self.browser.get(\"http://www.ivc-online.com\")\n",
    "      \n",
    "        time.sleep(5)\n",
    "        u_name=self.browser.find_element_by_id(\"dnn_ctr456_View_SignIn_tbEmail\")\n",
    "        u_name.send_keys(\"\")\n",
    "        u_name.send_keys(\"skhavul@uta.edu\")\n",
    "        time.sleep(5)\n",
    "        pwd=self.browser.find_element_by_id(\"dnn_ctr456_View_SignIn_tbPass\")\n",
    "        pwd.send_keys(\"\")\n",
    "            #time.sleep(10)\n",
    "        pwd.send_keys(\"susanna2992\")\n",
    "        time.sleep(7)\n",
    "        button = self.browser.wait.until(EC.element_to_be_clickable((By.ID, \"dnn_ctr456_View_SignIn_lbSignIn\")))\n",
    "        time.sleep(7)\n",
    "        button.click()\n",
    "        time.sleep(5)\n",
    "        \n",
    " \n",
    "        \n",
    "        u_search=self.browser.find_element_by_xpath('//*[@id=\"dnn_ctr447_View_Search_tbSearch\"]')\n",
    "        #u_name.send_keys(\"\")\n",
    "        u_search.send_keys('abc')\n",
    "        button1 = self.browser.wait.until(EC.element_to_be_clickable((By.ID, \"dnn_ctr447_View_Search_lbGo\")))\n",
    "        button1.click()\n",
    "        \n",
    "        \n",
    "        lobby_csv = open(r'C:/Users/DELL/Downloads/Book2.csv', 'r') # input\n",
    "        for each_line in lobby_csv:\n",
    "            lobbyfirm_id = each_line.strip()\n",
    "            \n",
    "            u_search2=self.browser.find_element_by_xpath('//*[@id=\"dnn_ctr462_View_Search_tbSearch\"]')\n",
    "            u_search2.send_keys(lobbyfirm_id)\n",
    "            button12 = self.browser.wait.until(EC.element_to_be_clickable((By.ID, \"dnn_ctr462_View_Search_lbGo\")))\n",
    "            button12.click()\n",
    "        \n",
    "            \n",
    "\n",
    "        \n",
    "            time.sleep(8)\n",
    "            try:\n",
    "            \n",
    "                like = self.browser.find_elements(By.CLASS_NAME,\"ASresults_TD\")\n",
    "                targt=like[0].find_elements(By.TAG_NAME,\"a\")\n",
    "                targt[0].click()\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "        \n",
    "            time.sleep(8)\n",
    "            self.browser.switch_to.window(self.browser.window_handles[1])\n",
    "            self.scrap(lobbyfirm_id)\n",
    "\n",
    "            \n",
    "    def scrap(self,lobbyfirm):\n",
    "        company_name=self.find_element_text('//*[@id=\"dnn_ctr499_View_CompanyCard_HeaderCard1_lFullName\"]')\n",
    "        if(self.check_company(company_name)!=True):\n",
    "            companies=dict()\n",
    "            data=[]\n",
    "            with open(\"firm_id.txt\",\"r\") as id_file:\n",
    "                company_id=int(id_file.read())\n",
    "            with open(\"firm_id.txt\",\"w\") as id_file:\n",
    "                company_id=company_id+1\n",
    "                id_file.write(str(company_id))\n",
    "            print (company_id)\n",
    "            data.append(company_id)\n",
    "            company_name=self.find_element_text('//*[@id=\"dnn_ctr499_View_CompanyCard_HeaderCard1_lFullName\"]')\n",
    "            data.append(company_name)\n",
    "            if company_name not in companies:\n",
    "                companies[company_name]=1\n",
    "            try:\n",
    "                company_website=self.browser.find_element_by_xpath('//*[@id=\"dnn_ctr499_View_CompanyCard_HeaderCard1_hlWebSite\"]/a').get_attribute(\"href\")\n",
    "            except:\n",
    "                company_website=\"null\"\n",
    "            data.append(company_website)\n",
    "            company_now_value=self.find_element_text('//*[@id=\"dnn_ctr499_View_CompanyCard_HeaderCard1_lnowcomp\"]')\n",
    "            data.append(company_now_value)\n",
    "            company_formerly_value=self.find_element_text('//*[@id=\"dnn_ctr499_View_CompanyCard_HeaderCard1_lCompName\"]')\n",
    "            data.append(company_formerly_value)\n",
    "            try:\n",
    "                last_updated=self.browser.find_element_by_xpath('//*[@id=\"dnn_ctr499_View_CompanyCard_lUpdateDate\"]').text.strip().replace(\"\\n\",\"\")    \n",
    "            except:\n",
    "                last_updated=\"null\"\n",
    "            data.append(last_updated)\n",
    "            \n",
    "            company_subtype=self.find_element_text('//*[@id=\"dnn_ctr499_View_CompanyCard_HeaderCard1_lCompSubType\"]')\n",
    "            data.append(company_subtype)\n",
    "            company_sector=self.find_element_text('//*[@id=\"dnn_ctr499_View_CompanyCard_HeaderCard1_lCompSector\"]')\n",
    "            data.append(company_sector)\n",
    "            company_status=self.find_element_text('//*[@id=\"dnn_ctr499_View_CompanyCard_HeaderCard1_lCompStatus\"]')\n",
    "            data.append(company_status)\n",
    "            company_stage=self.find_element_text('//*[@id=\"dnn_ctr499_View_CompanyCard_GeneralData1_lStage\"]')\n",
    "            data.append(company_stage)\n",
    "    \n",
    "            established_year=self.find_element_text('//*[@id=\"dnn_ctr499_View_CompanyCard_GeneralData1_lEstYear\"]')\n",
    "            data.append(established_year)\n",
    "            no_of_employees=self.find_element_text('//*[@id=\"dnn_ctr499_View_CompanyCard_GeneralData1_lNoemp\"]')\n",
    "            data.append(no_of_employees)\n",
    "            reg_no=self.find_element_text('//*[@id=\"dnn_ctr499_View_CompanyCard_GeneralData1_Lblreg\"]')\n",
    "            data.append(reg_no)\n",
    "            competitors=self.find_element_text('//*[@id=\"dnn_ctr499_View_CompanyCard_GeneralData1_tdCpmt\"]').strip()\n",
    "            data.append(competitors)\n",
    "\n",
    "    \n",
    "            tech=self.find_element_text('//*[@id=\"dnn_ctr499_View_CompanyCard_GeneralData1_lTech\"]').strip().replace(\"\\n\",\"\")\n",
    "            data.append(tech)\n",
    "            target_markets=self.find_element_text('//*[@id=\"dnn_ctr499_View_CompanyCard_GeneralData1_tdTargetedCustomers\"]')\n",
    "            data.append(target_markets)\n",
    "            manComp=self.find_element_text('//*[@id=\"dnn_ctr499_View_CompanyCard_GeneralData1_hfMangComp\"]')\n",
    "            data.append(manComp)\n",
    "            \n",
    "    \n",
    "            company_tabs=self.browser.find_elements_by_xpath('//*[@class=\"cc_menut links_CSS2\"]')\n",
    "            tab_index=0\n",
    "            for tab in range(len(company_tabs)):\n",
    "                company_tabs[tab_index].click()\n",
    "                time.sleep(10)\n",
    "                soup=BeautifulSoup(self.browser.page_source,\"html.parser\")\n",
    "                company_tabs=self.browser.find_elements_by_xpath('//*[@class=\"cc_menut links_CSS2\"]')\n",
    "                \n",
    "                tab_name=company_tabs[tab_index].text.strip().replace(\"\\n\",\"\")\n",
    "                try:\n",
    "                    directory = \"C:\\\\Users\\\\DELL\\\\Downloads{}_{}\".format(company_id,company_name)\n",
    "                except:\n",
    "                    directory = \"C:\\\\Users\\\\DELL\\\\Downloads{}\".format(company_id)\n",
    "                if not os.path.exists(directory):\n",
    "                    os.makedirs(directory)\n",
    "                if tab_name==\"General Data\":\n",
    "                    filepath = directory +\"\\\\\"+ tab_name + \".html\"\n",
    "                    fileId = open(filepath,'w',encoding='utf-8')\n",
    "                    cont = (soup.prettify())\n",
    "                    fileId.write(cont)\n",
    "                    fileId.close()\n",
    "                    self.scrap_general_data(soup,company_id,data,lobbyfirm)\n",
    "                if tab_name==\"Contact Info\":\n",
    "                    filepath = directory +\"\\\\\"+ tab_name + \".html\"\n",
    "                    fileId = open(filepath,'w',encoding='utf-8')\n",
    "                    cont = (soup.prettify())\n",
    "                    fileId.write(cont)\n",
    "                    fileId.close()\n",
    "                    self.scrap_contact(soup,company_id)\n",
    "                if tab_name==\"Management & Board\":\n",
    "                    filepath = directory +\"\\\\\"+ tab_name + \".html\"\n",
    "                    fileId = open(filepath,'w',encoding='utf-8')\n",
    "                    cont = (soup.prettify())\n",
    "                    fileId.write(cont)\n",
    "                    fileId.close()\n",
    "                    self.scrap_mgmnt(soup,company_id)\n",
    "                if tab_name==\"Investment Data\":\n",
    "                    filepath = directory +\"\\\\\"+ tab_name + \".html\"\n",
    "                    fileId = open(filepath,'w',encoding='utf-8')\n",
    "                    cont = (soup.prettify())\n",
    "                    fileId.write(cont)\n",
    "                    fileId.close()\n",
    "                    self.scrapInvestmentData(soup,company_id)\n",
    "                if tab_name==\"Portfolio Companies\":\n",
    "                    filepath = directory +\"\\\\\"+ tab_name + \".html\"\n",
    "                    fileId = open(filepath,'w',encoding='utf-8')\n",
    "                    cont = (soup.prettify())\n",
    "                    fileId.write(cont)\n",
    "                    fileId.close()\n",
    "                    self.scrap_pc(soup,company_id)\n",
    "                if tab_name==\"Financial Deals\":\n",
    "                    filepath = directory +\"\\\\\"+ tab_name + \".html\"\n",
    "                    fileId = open(filepath,'w',encoding='utf-8')\n",
    "                    cont = (soup.prettify())\n",
    "                    fileId.write(cont)\n",
    "                    fileId.close()\n",
    "                    self.scrap_finance(soup,company_id)\n",
    "                    self.scrap_finance_grant(soup,company_id)\n",
    "                    self.scrap_finance_ma(soup,company_id)\n",
    "                if tab_name==\"News & Multimedia\":\n",
    "                    filepath = directory +\"\\\\\"+ tab_name + \".html\"\n",
    "                    fileId = open(filepath,'w',encoding='utf-8')\n",
    "                    cont = (soup.prettify())\n",
    "                    fileId.write(cont)\n",
    "                    fileId.close()\n",
    "                    self.scrap_newsm(soup,company_id)\n",
    "                if tab_index<=len(company_tabs):\n",
    "                    tab_index=tab_index+1\n",
    "            self.browser.close()\n",
    "            self.browser.switch_to.window(self.browser.window_handles[0])\n",
    "            \n",
    "    def scrap_general_data(self,soup,company_id,data,lobby):\n",
    "        estimated_sales=[]\n",
    "        service_providers = []\n",
    "        product_ad_phases = []\n",
    "        affiliation =[]\n",
    "        relfund = []\n",
    "        estimated_sales=self.check_estimates_sales('//*[@id=\"dnn_ctr499_View_CompanyCard_GeneralData1_gd_annSale\"]')\n",
    "\n",
    "        service_providers=self.check_service_providers('//*[@id=\"dnn_ctr499_View_CompanyCard_GeneralData1_tdSrv\"]')\n",
    "        product_ad_phases=self.check_service_providers('//*[@id=\"dnn_ctr499_View_CompanyCard_GeneralData1_tdProduct\"]')\n",
    "        affiliation = self.check_service_providers('//*[@id=\"dnn_ctr499_View_CompanyCard_GeneralData1_tdAff\"]')\n",
    "        relfund = self.check_service_providers('//*[@id=\"dnn_ctr499_View_CompanyCard_GeneralData1_tdFund\"]')\n",
    "        if len(estimated_sales)!=0:\n",
    "            try:\n",
    "                estimated_sales.insert(0,company_id)\n",
    "                with open(\"nonvc_active_General_data_estimated_sales.tsv\",'a+') as file_object:\n",
    "                    file_object.write(\"\\n{}\".format(estimated_sales))\n",
    "#                    for row in range(len(estimated_sales)):\n",
    "#                        file_object.write(\"{}\\t{}\\n\".format(estimated_sales[row],estimated_sales[row+1]))\n",
    "            except:\n",
    "                fileMissed = open('missed.txt','a+')\n",
    "                fileMissed.write('{},{}\\n'.format(company_id,'gdes'))\n",
    "                fileMissed.close()\n",
    "                print (\"tdtags gdes\")\n",
    "                    \n",
    "                        \n",
    "                    \n",
    "        if len(product_ad_phases)!=0:\n",
    "            try:\n",
    "                product_ad_phases.insert(0,company_id)\n",
    "                with open(\"nonvc_active_General_data_productsandPhases.tsv\",'a+') as file_object:\n",
    "                    file_object.write(\"\\n{}\".format(product_ad_phases))\n",
    "#                    for row in range(0,len(product_ad_phases)):\n",
    "#                        if(row%3<>0):\n",
    "#                            file_object.write(\"{}\\t{}\\t{}\\t{}\\n\".format(product_ad_phases[row]))\n",
    "            except:\n",
    "                fileMissed = open('missed.txt','a+')\n",
    "                fileMissed.write('{},{}\\n'.format(company_id,'gdpp'))\n",
    "                fileMissed.close()\n",
    "                print (\"tdtags gdpp\")\n",
    "                    \n",
    "        if len(service_providers)!=0:\n",
    "            try:\n",
    "                service_providers.insert(0,company_id)\n",
    "                with open(\"nonvc_active_General_Data_Service_Providers.tsv\",'a+') as file_object:\n",
    "                    file_object.write(\"\\n{}\".format(service_providers))\n",
    "                    \n",
    "#                    for row in range(0,len(service_providers)):\n",
    "#                        if(row%3<>0):\n",
    "#                            file_object.write(\"{}\\t\".format(service_providers[row]))\n",
    "            except:\n",
    "                fileMissed = open('missed.txt','a+')\n",
    "                fileMissed.write('{},{}\\n'.format(company_id,'gdsp'))\n",
    "                fileMissed.close()\n",
    "                print (\"tdtags gdsp\")\n",
    "                \n",
    "        if len(relfund)!=0:\n",
    "            try:\n",
    "                relfund.insert(0,company_id)\n",
    "            \n",
    "                with open(\"nonvc_active_General_Data_Related_Funds.tsv\",'a+') as file_object:\n",
    "                    file_object.write(\"\\n{}\".format(relfund))\n",
    "                \n",
    "#                    for row in range(0,len(relfund)):\n",
    "#                        if(row%3<>0):\n",
    "#                            file_object.write(\"{}\\t\".format(relfund[row]))\n",
    "            except:\n",
    "                fileMissed = open('missed.txt','a+')\n",
    "                fileMissed.write('{},{}\\n'.format(company_id,'gdrf'))\n",
    "                fileMissed.close()\n",
    "                print (\"tdtags gdrf\")\n",
    "                        \n",
    "        if len(affiliation)!=0:\n",
    "            try:\n",
    "                affiliation.insert(0,company_id)\n",
    "                #print affiliation\n",
    "                with open(\"nonvc_active_General_Data_Affiliation.tsv\",'a+') as file_object:\n",
    "                    file_object.write(\"\\n{}\".format(affiliation))\n",
    "                    \n",
    "#                    for row in range(0,len(affiliation)):\n",
    "#                        if(row%3<>0):\n",
    "#                            file_object.write(\"{}\\t\".format(affiliation[row]))\n",
    "            except:\n",
    "                fileMissed = open('missed.txt','a+')\n",
    "                fileMissed.write('{},{}\\n'.format(company_id,'gdaf'))\n",
    "                fileMissed.close()\n",
    "                print (\"tdtags gdaf\")\n",
    "                    \n",
    "        with open(\"nonvc_active_General_Data.tsv\",'a+') as file_object:\n",
    "            file_object.write(\"\\n\")\n",
    "            description=self.find_element_text('//*[@id=\"dnn_ctr499_View_CompanyCard_GeneralData1_lDisc\"]').strip().replace(\"\\n\",\"\")\n",
    "            curPortfolioComp = self.find_element_text('//*[@id=\"dnn_ctr499_View_CompanyCard_GeneralData1_lPortComp\"]').strip().replace(\"\\n\",\"\")\n",
    "            data.append(curPortfolioComp)\n",
    "            data.append(description)\n",
    "            data.append(lobby)\n",
    "            for datum in range(len(data)):\n",
    "                if datum!=len(data)-1:\n",
    "                    file_object.write(\"{}\\t\".format(data[datum]))\n",
    "                else:\n",
    "                    file_object.write(\"{}\".format(data[datum]))\n",
    "\n",
    "    def scrap_contact(self,soup,company_id):\n",
    "        try:\n",
    "            div_tag=soup.find_all(\"div\",{\"id\":\"picContactDiv\"})\n",
    "            td_tags = div_tag[0].find_all(\"td\",{\"class\":\"cctd_box_mdl\"})\n",
    "            for td_tag in range(len(td_tags)):\n",
    "                span_tags=td_tags[td_tag].find_all(\"span\")\n",
    "                a_tags=td_tags[td_tag].find_all(\"a\")\n",
    "                contact_tab=dict()\n",
    "                for span_tag in range(len(span_tags)):\n",
    "                    if \":\" in (span_tags[span_tag].text.strip().replace(\"\\n\",\"\")):\n",
    "                        if str(span_tags[span_tag].text.strip().replace(\"\\n\",\"\")) not in contact_tab:\n",
    "                            if str(span_tags[span_tag].text.strip().replace(\"\\n\",\"\")) is not \"\":\n",
    "                                if \"Main Address\" in str(span_tags[span_tag].text.strip().replace(\"\\n\",\"\")):\n",
    "                                    contact_tab[str(span_tags[span_tag].text.split(\":\")[0].strip().replace(\"\\n\",\"\"))]=\"\"\n",
    "                                else:\n",
    "                                    contact_tab[str(span_tags[span_tag].text.strip().replace(\"\\n\",\"\"))]=\"\"\n",
    "                                if \"Main Address\" in str(span_tags[span_tag].text.strip().replace(\"\\n\",\"\")):\n",
    "                                    last_tag_main,main_addr=span_tags[span_tag].text.split(\":\")\n",
    "                                    contact_tab[last_tag_main]=str(main_addr)\n",
    "                                else:\n",
    "                                    last_tag=str(span_tags[span_tag].text.strip().replace(\"\\n\",\"\"))\n",
    "                                continue\n",
    "                    if \"Main Address\" in (span_tags[span_tag].text.strip().replace(\"\\n\",\"\")):\n",
    "                        contact_tab[last_tag]=str(main_addr)#str(span_tags[span_tag].text.split(\":\")[1].strip().replace(\"\\n\",\"\"))\n",
    "                    else:\n",
    "                        contact_tab[last_tag]=(span_tags[span_tag].text.strip().replace(\"\\n\",\"\"))            \n",
    "                emailId = []\n",
    "                for a_tag in range(len(a_tags)):\n",
    "                    if \".com\" in a_tags[a_tag].text.strip().replace(\"\\n\",\"\"):\n",
    "                        emailId.append(a_tags[a_tag].text.strip().replace(\"\\n\",\"\"))\n",
    "                if len(emailId) == 2:                \n",
    "                    contact_tab['Company E-mail:']=emailId[1]\n",
    "                    contact_tab['Contact E-Mail:']=emailId[0]\n",
    "                elif len(emailId) == 1:\n",
    "                    contact_tab['Contact E-Mail:']=emailId[0]\n",
    "                with open(\"nonvc_active_Contact_Info.tsv\",'a+') as file_object:\n",
    "                    file_object.write(\"\\n\")\n",
    "                    header=[\"Contact Person:\",\"Contact E-Mail:\",\"Company E-mail:\",\"Telephone:\",\"Address:\",\"Main Address:\",\"Branch Address:\",\"POB:\",\"City:\",\"Country:\",\"Zip:\",\"Fax:\"]\n",
    "                    file_object.write(str(company_id)+\"\\t\")\n",
    "                    for heading in header:\n",
    "                        if heading not in contact_tab:\n",
    "                            contact_tab[heading]=\"\"\n",
    "                        if heading in contact_tab:\n",
    "                            file_object.write('{}{}'.format(contact_tab[heading],\"\\t\"))\n",
    "        except:\n",
    "            fileMissed = open('missed.txt','a+')\n",
    "            fileMissed.write('{},{}\\n'.format(company_id, 'ci'))\n",
    "            fileMissed.close()\n",
    "            print (\"tdtags ci\")\n",
    "            \n",
    "    def scrap_mgmnt(self,soup,company_id):\n",
    "        table_mgmnt_tag=soup.find_all(\"table\",{\"id\":\"dnn_ctr499_View_CompanyCard_ManagementBoard1_mb_Mang\"})\n",
    "        self.scrp_mgmnt_table(table_mgmnt_tag,company_id,\"mgmnt\")\n",
    "        table_board_tag=soup.find_all(\"table\",{\"id\":\"dnn_ctr499_View_CompanyCard_ManagementBoard1_mb_Board\"})\n",
    "        self.scrp_mgmnt_table(table_board_tag,company_id,\"board\")\n",
    "        \n",
    "    def scrp_mgmnt_table(self,table_tag,company_id,company_name):\n",
    "        try:\n",
    "            if(len(table_tag)>0):        \n",
    "                td_tags=table_tag[0].find_all(\"td\",{\"class\":\"cctd_box_mdl\"})\n",
    "                tr_tags=td_tags[0].find_all(\"tr\")\n",
    "                \n",
    "                with open(\"nonvc_active_\"+company_name+\"_data.tsv\",'a+') as file_object:\n",
    "                    file_object.write(\"\\n\"+str(company_id)+\"\\t\")\n",
    "                    data_tags_headings = tr_tags[0].find_all(\"td\")\n",
    "                    for head in data_tags_headings:\n",
    "                        file_object.write(head.text.strip().replace(\"\\n\",\"\")+\"\\t\")\n",
    "                    for row in range(1,len(tr_tags)):\n",
    "                        file_object.write(\"\\n\"+str(company_id)+\"\\t\")\n",
    "                        data_tags=tr_tags[row].find_all(\"td\")\n",
    "                        \n",
    "                    \n",
    "                        #file_object.write(\"\\n\"+str(company_id)+\"\\t\")\n",
    "                        \n",
    "                                       \n",
    "                    \n",
    "                    \n",
    "                        for data in range(0,len(data_tags)):\n",
    "                        \n",
    "                            file_object.write(data_tags[data].text.strip().replace(\"\\n\",\"\")+\"\\t\")\n",
    "        except:\n",
    "            fileMissed = open('missed.txt','a+')\n",
    "            fileMissed.write('{},{}\\n'.format(company_id, company_name))\n",
    "            fileMissed.close()\n",
    "            print (\"tdtags mn\")\n",
    "    def scrap_finance_ma(self,soup,company_id):\n",
    "        \n",
    "         \n",
    "        try:\n",
    "            table_tag=soup.find_all(\"table\",{\"id\":\"dnn_ctr499_View_CompanyCard_Deals1_d_MergeAcq\"})\n",
    "            \n",
    "            td_tags=table_tag[0].find_all(\"td\",{\"class\":\"cctd_box_mdl\"})\n",
    "                \n",
    "            tr_tags=td_tags[0].find(\"tbody\").find_all('tr',recursive=False)\n",
    "                #print len(tr_tags)\n",
    "                    \n",
    "            st = \"\"\n",
    "            for row in td_tags:\n",
    "                row1 = row.find_all('td')\n",
    "                for r in row1:\n",
    "                    if \"Mergers\" not in r.text.strip() and \"Deal Date\" not in r.text.strip() and \"Type of Deal\" not in r.text.strip() and \"Amount\" not in r.text.strip() and \"Remarks\" not in r.text.strip():\n",
    "                        st += r.text.strip() + \"&& \"\n",
    "            \n",
    "            with open('nonvc_active_Financial_Deals_ma.tsv','a+') as file_object: \n",
    "                file_object.write(\"\\n\"+str(company_id)+\"\\t\"+st)\n",
    "                    \n",
    "                    \n",
    "        except:\n",
    "            fileMissed = open('missed.txt','a+')\n",
    "            fileMissed.write('{},{}\\n'.format(company_id,'fdma'))\n",
    "            fileMissed.close()\n",
    "            print (\"tdtags fd\")\n",
    "            \n",
    "    def scrap_finance_grant(self,soup,company_id):\n",
    "        \n",
    "        try:\n",
    "            table_tag=soup.find_all(\"table\",{\"id\":\"dnn_ctr499_View_CompanyCard_Deals1_d_Grant\"})\n",
    "            \n",
    "            td_tags=table_tag[0].find_all(\"td\",{\"class\":\"cctd_box_mdl\"})\n",
    "                \n",
    "            tr_tags=td_tags[0].find(\"tbody\").find_all('tr',recursive=False)\n",
    "                #print len(tr_tags)\n",
    "                    \n",
    "            st = \"\"\n",
    "            for row in td_tags:\n",
    "                row1 = row.find_all('td')\n",
    "                for r in row1:\n",
    "                    if \"R&D Grant\" not in r.text.strip() and \"Investor\" not in r.text.strip() and \"Type\" not in r.text.strip() and \"Amount\" not in r.text.strip() and \"Remarks\" not in r.text.strip():\n",
    "                        st += r.text.strip() + \"&& \"\n",
    "            \n",
    "            with open('nonvc_active_Financial_Deals_rd.tsv','a+') as file_object:\n",
    "                file_object.write(\"\\n\"+str(company_id)+\"\\t\"+st)\n",
    "                    \n",
    "                    \n",
    "        except:\n",
    "            fileMissed = open('missed.txt','a+')\n",
    "            fileMissed.write('{},{}\\n'.format(company_id,'fdg'))\n",
    "            fileMissed.close()\n",
    "            print (\"tdtags fd\")\n",
    "            \n",
    "    def scrap_finance(self,soup,company_id):\n",
    "        with open('nonvc_active_finance_deals.tsv','a+') as file_object:  \n",
    "            \n",
    "            table_tag=soup.find_all(\"table\",{\"id\":\"dnn_ctr499_View_CompanyCard_Deals1_d_Financing\"})\n",
    "            try:\n",
    "                td_tags=table_tag[0].find_all(\"td\",{\"class\":\"cctd_box_mdl\"})\n",
    "                total_capital=self.find_element_text(\"//*[@id='dnn_ctr499_View_CompanyCard_Deals1_lblTotal']\")\n",
    "            \n",
    "                nested_table=td_tags[0].find_all(\"table\")\n",
    "                #print nested_table[0]\n",
    "                tr_tags=nested_table[0].find(\"tbody\").find_all('tr',recursive=False)\n",
    "                #print len(tr_tags)\n",
    "                for row in tr_tags:\n",
    "                    #print row.get('class')\n",
    "                    if row.has_attr('class'):\n",
    "                        if row.get('class')[0]=='cssFinnDeals':\n",
    "                            round_name=row.find_all('td')[0].text.strip()\n",
    "                            try:\n",
    "                                date=row.find_all('td')[1].find_all('span')[1].text.strip()\n",
    "                            except:\n",
    "                                date = \"null\"\n",
    "                            file_object.write(\"\\n\"+str(company_id)+\"\\t\")\n",
    "                            file_object.write(str(total_capital)+\"\\t\")\n",
    "                            file_object.write(str(round_name)+\"\\t\")\n",
    "                            file_object.write(str(date)+\"\\t\")\n",
    "                    elif row.has_attr(\"id\") and not row.has_attr('class'):\n",
    "                    \n",
    "                        table=row.find_all('table')\n",
    "                        table_rows=table[0].find_all('tr')\n",
    "                        for table_row in range(2,len(table_rows)):\n",
    "                            datum=table_rows[table_row].find_all('td')\n",
    "                            for data in datum:\n",
    "                                file_object.write(str(data.text.strip().replace(\"\\n\",\"\"))+\"\\t\")\n",
    "                        \n",
    "                    else:\n",
    "                        print (\"outside\")\n",
    "                    \n",
    "                        amount=''\n",
    "                        stage=''\n",
    "                        valuation=''\n",
    "                        td_tags=row.find_all('td')\n",
    "                        for tags in td_tags:\n",
    "                            span_tags=tags.find_all('span')\n",
    "                        \n",
    "                            for span_tag in range(len(span_tags)):\n",
    "                                if(span_tags[span_tag].text.strip()=='Total Amount:'):\n",
    "                                    amount=span_tags[span_tag+1].text.strip()   \n",
    "                                    #print amount\n",
    "                                if(span_tags[span_tag].text.strip()=='Stage:'):\n",
    "                                    stage=span_tags[span_tag+1].text.strip()\n",
    "                                    #print stage\n",
    "                                if(span_tags[span_tag].text.strip()=='Valuation:'):\n",
    "                                    valuation=span_tags[span_tag+1].text.strip()\n",
    "                                    #print valuation\n",
    "                        file_object.write(amount+\"\\t\")\n",
    "                        file_object.write(stage+\"\\t\")\n",
    "                        file_object.write(valuation+\"\\t\")\n",
    "                    \n",
    "            except:\n",
    "                fileMissed = open('missed.txt','a+')\n",
    "                fileMissed.write('{},{}\\n'.format(company_id,'fd'))\n",
    "                fileMissed.close()\n",
    "                print (\"tdtags fd\")\n",
    "                \n",
    "                    \n",
    "#            except:\n",
    "#                fileMissed = open('missed.txt','a+')\n",
    "#                fileMissed.write('{},{}\\n'.format(company_id,'fd'))\n",
    "#                fileMissed.close()\n",
    "#                print (\"tdtags fd\"\n",
    "                \n",
    "    def getText(self,spanTag):\n",
    "        if spanTag is not None:\n",
    "            return str(spanTag.get_text().strip().replace(\"\\n\",\"\"))\n",
    "        else:\n",
    "            return \"None\"\n",
    "            \n",
    "    def scrapInvestmentData(self,soup,company_id):\n",
    "        with open('nonvc_active_Investment_Data.tsv','a+') as file_object:\n",
    "            try:\n",
    "                capRemarks=self.getText(soup.find(\"span\",{\"id\": \"dnn_ctr499_View_CompanyCard_InvestmentData1_lRemark\"}))\n",
    "                typofFinance=self.getText(soup.find(\"span\",{\"id\": \"dnn_ctr499_View_CompanyCard_InvestmentData1_lType\"}))\n",
    "                stgPref=self.getText(soup.find(\"span\",{\"id\": \"dnn_ctr499_View_CompanyCard_InvestmentData1_lStage\"}))\n",
    "                indPref=self.getText(soup.find(\"span\",{\"id\": \"dnn_ctr499_View_CompanyCard_InvestmentData1_lIndustry\"}))\n",
    "                phasePref=self.getText(soup.find(\"span\",{\"id\": \"dnn_ctr499_View_CompanyCard_InvestmentData1_lPhase\"}))\n",
    "                maxInv=self.getText(soup.find(\"span\",{\"id\": \"dnn_ctr499_View_CompanyCard_InvestmentData1_lMax\"}))\n",
    "                minInv=self.getText(soup.find(\"span\",{\"id\": \"dnn_ctr499_View_CompanyCard_InvestmentData1_lMin\"}))\n",
    "                manCap=self.getText(soup.find(\"span\",{\"id\": \"dnn_ctr499_View_CompanyCard_InvestmentData1_lMng\"}))\n",
    "                file_object.write(\"\\n\"+str(company_id)+\"\\t\"+manCap+\"\\t\"+capRemarks+\"\\t\"+maxInv+\"\\t\"+minInv+\"\\t\"+typofFinance+\"\\t\"+stgPref+\"\\t\"+phasePref+\"\\t\"+indPref+\"\\t\")\n",
    "                secSeg = \"\"\n",
    "                table_tag=soup.find_all(\"table\",{\"id\":\"dnn_ctr499_View_CompanyCard_InvestmentData1_di_sector\"})\n",
    "                \n",
    "                if table_tag != []:\n",
    "                    td_tags=table_tag[0].find_all(\"td\",{\"class\":\"cctd_box_mdl\"})\n",
    "                    nested_table=td_tags[0].find_all(\"table\")\n",
    "                    tr_tags=nested_table[0].find(\"tbody\").find_all('tr')\n",
    "                    for tr_tag in range(1,len(tr_tags)):\n",
    "                        td_tags=tr_tags[tr_tag].find_all('td',{\"class\":\"cc_t_u_txt\"})\n",
    "                    \n",
    "                        for td_tag in td_tags:\n",
    "                            secSeg += (td_tag.text.strip().replace(\"\\n\",\"\"))\n",
    "                    file_object.write(secSeg+\"\\t\")\n",
    "            except:\n",
    "                fileMissed = open('missed.txt','a+')\n",
    "                fileMissed.write('{},{}\\n'.format(company_id,'id'))\n",
    "                fileMissed.close()\n",
    "                print (\"tdtags id\")\n",
    "            \n",
    "                    \n",
    "    def scrap_newsm(self,soup,company_id):\n",
    "        with open('nonvc_active_News_and_Multimedia.tsv','a+') as file_object:  \n",
    "            \n",
    "            table_tag=soup.find_all(\"table\",{\"id\":\"dnn_ctr499_View_CompanyCard_NewsMultimedia1_tPR\"})\n",
    "            try:\n",
    "                if table_tag != []:\n",
    "                    td_tags=table_tag[0].find_all(\"td\",{\"class\":\"cctd_box_mdl\"})\n",
    "                    nested_table=td_tags[0].find_all(\"table\")\n",
    "                    tr_tags=nested_table[0].find(\"tbody\").find_all('tr')\n",
    "                    td_tags_heading = tr_tags[0].find_all('td')\n",
    "                    file_object.write(\"\\n\"+str(company_id)+\"\\t\")\n",
    "                    for head in td_tags_heading:\n",
    "                        file_object.write(head.text.strip().replace(\"\\n\",\"\")+\"\\t\")\n",
    "                    for tr_tag in range(1,len(tr_tags)):\n",
    "                        td_tags=tr_tags[tr_tag].find_all('td')\n",
    "                        file_object.write(\"\\n\"+str(company_id)+\"\\t\")\n",
    "                        for td_tag in range(0,len(td_tags)):\n",
    "                            file_object.write(td_tags[td_tag].text.strip().replace(\"\\n\",\"\")+\"\\t\")\n",
    "            except:\n",
    "                fileMissed = open('missed.txt','a+')\n",
    "                fileMissed.write('{},{}\\n'.format(company_id,'newsmm'))\n",
    "                fileMissed.close()\n",
    "                print (\"tdtags newsmm\")\n",
    "    \n",
    "    def scrap_pc(self,soup,company_id):\n",
    "        with open('nonvc_active_Current_Portfolio_Companies.tsv','a+') as file_object:  \n",
    "            \n",
    "            table_tag=soup.find_all(\"table\",{\"id\":\"dnn_ctr499_View_CompanyCard_Portfolio1_pc_currPort\"})\n",
    "            try:\n",
    "                if table_tag != []:\n",
    "                    td_tags=table_tag[0].find_all(\"td\",{\"class\":\"cctd_box_mdl\"})\n",
    "                    nested_table=td_tags[0].find_all(\"table\")\n",
    "                    tr_tags=nested_table[0].find(\"tbody\").find_all('tr')\n",
    "                    td_tags_heading = tr_tags[0].find_all('td')\n",
    "                    file_object.write(\"\\n\"+str(company_id)+\"\\t\")\n",
    "                    for head in td_tags_heading:\n",
    "                        file_object.write(head.text.strip().replace(\"\\n\",\"\")+\"\\t\")\n",
    "                    for tr_tag in range(1,len(tr_tags)):\n",
    "                        td_tags=tr_tags[tr_tag].find_all('td')\n",
    "                        file_object.write(\"\\n\"+str(company_id)+\"\\t\")\n",
    "                        for td_tag in range(0,len(td_tags)):\n",
    "                            file_object.write(td_tags[td_tag].text.strip().replace(\"\\n\",\"\")+\"\\t\")\n",
    "            except:\n",
    "                fileMissed = open('missed.txt','a+')\n",
    "                fileMissed.write('{},{}\\n'.format(company_id,'current pc'))\n",
    "                fileMissed.close()\n",
    "                print (\"tdtags current pc\")\n",
    "                \n",
    "        with open('nonvc_active_Current_VC_Fund_Portfolio.tsv','a+') as file_object:  \n",
    "            \n",
    "            table_tag=soup.find_all(\"table\",{\"id\":\"dnn_ctr499_View_CompanyCard_Portfolio1_pc_VCPort\"})\n",
    "            \n",
    "            try:\n",
    "                if table_tag != []:\n",
    "                    td_tags=table_tag[0].find_all(\"td\",{\"class\":\"cctd_box_mdl\"})\n",
    "                    nested_table=td_tags[0].find_all(\"table\")\n",
    "                    tr_tags=nested_table[0].find(\"tbody\").find_all('tr')\n",
    "                    td_tags_heading = tr_tags[0].find_all('td')\n",
    "                    file_object.write(\"\\n\"+str(company_id)+\"\\t\")\n",
    "                    for head in td_tags_heading:\n",
    "                        file_object.write(head.text.strip().replace(\"\\n\",\"\")+\"\\t\")\n",
    "                    for tr_tag in range(1,len(tr_tags)):\n",
    "                        td_tags=tr_tags[tr_tag].find_all('td')\n",
    "                        file_object.write(\"\\n\"+str(company_id)+\"\\t\")\n",
    "                        for td_tag in range(0,len(td_tags)):\n",
    "                            file_object.write(td_tags[td_tag].text.strip().replace(\"\\n\",\"\")+\"\\t\")\n",
    "            except:\n",
    "                fileMissed = open('missed.txt','a+')\n",
    "                fileMissed.write('{},{}\\n'.format(company_id,'current vc'))\n",
    "                fileMissed.close()\n",
    "                print (\"tdtags current vc\")\n",
    "                \n",
    "        with open('nonvc_active_Exists.tsv','a+') as file_object:  \n",
    "            \n",
    "            table_tag=soup.find_all(\"table\",{\"id\":\"dnn_ctr499_View_CompanyCard_Portfolio1_pc_Exit\"})\n",
    "            \n",
    "            try:   \n",
    "                if table_tag != []:\n",
    "                    td_tags=table_tag[0].find_all(\"td\",{\"class\":\"cctd_box_mdl\"})\n",
    "                    nested_table=td_tags[0].find_all(\"table\")\n",
    "                    tr_tags=nested_table[0].find(\"tbody\").find_all('tr')\n",
    "                    td_tags_heading = tr_tags[0].find_all('td')\n",
    "                    file_object.write(\"\\n\"+str(company_id)+\"\\t\")\n",
    "                    for head in td_tags_heading:\n",
    "                        file_object.write(head.text.strip().replace(\"\\n\",\"\")+\"\\t\")\n",
    "                    for tr_tag in range(1,len(tr_tags)):\n",
    "                        td_tags=tr_tags[tr_tag].find_all('td')\n",
    "                        file_object.write(\"\\n\"+str(company_id)+\"\\t\")\n",
    "                        for td_tag in range(0,len(td_tags)):\n",
    "                            file_object.write(td_tags[td_tag].text.strip().replace(\"\\n\",\"\")+\"\\t\")\n",
    "            except:\n",
    "                fileMissed = open('missed.txt','a+')\n",
    "                fileMissed.write('{},{}\\n'.format(company_id,'exit'))\n",
    "                fileMissed.close()\n",
    "                print (\"tdtags exit\")\n",
    "                \n",
    "        with open('nonvc_active_Historic_Portfolio.tsv','a+') as file_object:  \n",
    "            \n",
    "            table_tag=soup.find_all(\"table\",{\"id\":\"dnn_ctr499_View_CompanyCard_Portfolio1_pc_Historic\"})\n",
    "            \n",
    "            try:\n",
    "                if table_tag != []:\n",
    "                    td_tags=table_tag[0].find_all(\"td\",{\"class\":\"cctd_box_mdl\"})\n",
    "                    nested_table=td_tags[0].find_all(\"table\")\n",
    "                    tr_tags=nested_table[0].find(\"tbody\").find_all('tr')\n",
    "                    td_tags_heading = tr_tags[0].find_all('td')\n",
    "                    file_object.write(\"\\n\"+str(company_id)+\"\\t\")\n",
    "                    for head in td_tags_heading:\n",
    "                        file_object.write(head.text.strip().replace(\"\\n\",\"\")+\"\\t\")\n",
    "                    for tr_tag in range(1,len(tr_tags)):\n",
    "                        td_tags=tr_tags[tr_tag].find_all('td')\n",
    "                        file_object.write(\"\\n\"+str(company_id)+\"\\t\")\n",
    "                        for td_tag in range(0,len(td_tags)):\n",
    "                            file_object.write(td_tags[td_tag].text.strip().replace(\"\\n\",\"\")+\"\\t\")\n",
    "            except:\n",
    "                fileMissed = open('missed.txt','a+')\n",
    "                fileMissed.write('{},{}\\n'.format(company_id,'historic'))\n",
    "                fileMissed.close()\n",
    "                print (\"tdtags historic\")\n",
    "            \n",
    "                    \n",
    "    def find_element_text(self,element_xpath):\n",
    "        if len(self.browser.find_elements_by_xpath(element_xpath))!=0:\n",
    "            return self.browser.find_element_by_xpath(element_xpath).text\n",
    "        else:\n",
    "            return \"null\"\n",
    "    def check_company(self,cmp_name):\n",
    "        with open(\"nonvc_active_General_Data.tsv\",'a+') as file_object:\n",
    "            file_data=file_object.read().split(\"\\n\")\n",
    "            check_value=False\n",
    "            for line in file_data:\n",
    "                try:\n",
    "                    dv = line.split(\"\\t\")[1]\n",
    "                    if dv==cmp_name:\n",
    "                        check_value=True\n",
    "                        break\n",
    "                    else:\n",
    "                        check_value=False\n",
    "                except:\n",
    "                    continue\n",
    "            return check_value\n",
    "                \n",
    "    def check_estimates_sales(self,path):\n",
    "        values=[]\n",
    "        if len(self.browser.find_elements_by_xpath(path))!=0:\n",
    "                datas=self.browser.find_element_by_xpath(path).find_elements(By.TAG_NAME,\"td\")\n",
    "                for td in datas:\n",
    "                    if td.text.strip() not in [\"&nbsp;\",\"Service Providers\",\"Year\",\"Amount\"]:\n",
    "                        values.append(td.text.strip().replace(\"\\n\",\"\"))\n",
    "        return values\n",
    "        \n",
    "    def check_service_providers(self,path):\n",
    "        values=[]\n",
    "        if len(self.browser.find_elements_by_xpath(path))!=0:\n",
    "                datas=self.browser.find_element_by_xpath(path).find_elements(By.TAG_NAME,\"tr\")\n",
    "                for tr in range(1,len(datas)):\n",
    "                    try:\n",
    "                        td=datas[tr].find_elements(By.TAG_NAME,\"td\")\n",
    "                        for data in range(len(td)):\n",
    "                            if td[data].text.strip() not in [\"&nbsp;\",\"Service Providers\",\"Company Name\",\"Type\",\"\",\"Product\",\"Phase\",\"Fund Name\",\"Fund Capital($m)\",\"Status\",\"Remarks\"]:\n",
    "                                values.append(td[data].text.strip())\n",
    "                    except:\n",
    "                        continue\n",
    "        return values\n",
    "\n",
    "            \n",
    "\n",
    "if __name__=='__main__':\n",
    "    #try:\n",
    "        scraper=scraping()\n",
    "        scraper.openWebsite()\n",
    "    #except Exception as e:\n",
    "     #   print e\n",
    "\n",
    "              \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
